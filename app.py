import streamlit as st
import pandas as pd
import numpy as np
import shap
import tensorflow as tf
import matplotlib.pyplot as plt
from matplotlib import rcParams
import os
import glob
import json

# é…ç½® matplotlib
rcParams['font.family'] = 'sans-serif'
rcParams['font.size'] = 10

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ¨¡å‹é¢„æµ‹å¯è§†åŒ–",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ CSS æ ·å¼
st.markdown("""
    <style>
    .main {background-color: #f8f9fa;}
    .stButton>button {background-color: #007bff; color: white; border-radius: 8px;}
    .stNumberInput>label {font-weight: bold; color: #2c3e50;}
    .sidebar .sidebar-content {background-color: #e9ecef;}
    h1 {color: #2c3e50; text-align: center;}
    h2 {color: #34495e; border-bottom: 2px solid #17a2b8; padding-bottom: 5px;}
    </style>
""", unsafe_allow_html=True)

# æ ‡é¢˜å’Œç®€ä»‹
st.title("æ¨¡å‹é¢„æµ‹å¯è§†åŒ–")
st.markdown("""
    æœ¬å·¥å…·ä½¿ç”¨ç‰¹å¾æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶é€šè¿‡ SHAP å¯è§†åŒ–æä¾›æœºç†è§£é‡Šã€‚
    åœ¨ä¾§è¾¹æ è°ƒæ•´ç‰¹å¾å€¼ï¼Œè§‚å¯Ÿé¢„æµ‹ç»“æœå’Œ SHAP å€¼çš„å˜åŒ–ã€‚
""")

# æŸ¥æ‰¾ Excel æ–‡ä»¶
def get_excel_file():
    excel_files = glob.glob('data/*.xlsx')
    if len(excel_files) != 1:
        st.error("data æ–‡ä»¶å¤¹ä¸­åº”æœ‰ä¸”ä»…æœ‰ä¸€ä¸ª Excel æ–‡ä»¶ã€‚")
        return None
    return excel_files[0]

# æŸ¥æ‰¾ JSON æ–‡ä»¶
def get_json_file():
    json_files = glob.glob('data/*.json')
    if len(json_files) != 1:
        st.error("data æ–‡ä»¶å¤¹ä¸­åº”æœ‰ä¸”ä»…æœ‰ä¸€ä¸ª JSON æ–‡ä»¶ã€‚")
        return None
    return json_files[0]

# åŠ è½½å½’ä¸€åŒ–æ–¹æ³•
@st.cache_data
def load_norm_method():
    json_file = get_json_file()
    if json_file is None:
        return 'none'
    try:
        with open(json_file, 'r') as f:
            config = json.load(f)
        norm_method = config.get('input_norm', 'none')
        if norm_method not in ['ext', 'avg', 'avgext', 'avgstd', 'none']:
            st.error(f"JSON æ–‡ä»¶ä¸­çš„å½’ä¸€åŒ–æ–¹æ³•æ— æ•ˆï¼š{norm_method}ã€‚ä½¿ç”¨ 'none'ã€‚")
            return 'none'
        return norm_method
    except Exception as e:
        st.error(f"è¯»å– JSON æ–‡ä»¶å‡ºé”™ï¼š{str(e)}ã€‚ä½¿ç”¨ 'none'ã€‚")
        return 'none'

# åŠ è½½èƒŒæ™¯æ•°æ®å’Œå½’ä¸€åŒ–å‚æ•°
@st.cache_data
def load_background_data():
    excel_file = get_excel_file()
    if excel_file is None:
        return None, None
    df = pd.read_excel(excel_file)
    features = df.iloc[:, :-2]  # æ’é™¤æœ€å2åˆ—ï¼ˆç›®æ ‡åˆ—ï¼‰
    param = {
        'mean': features.mean(),
        'max': features.max(),
        'std': features.std()
    }
    return features, param

# ç¡®å®šæ¨¡å‹ç±»å‹å’Œåˆ†ç±»æ ‡ç­¾
@st.cache_data
def determine_model_type():
    excel_file = get_excel_file()
    if excel_file is None:
        return None, None
    df = pd.read_excel(excel_file)
    target_col = df.iloc[:, -1]  # è·å–ç›®æ ‡åˆ—
    unique_values = target_col[1:].nunique()  # æ’é™¤è¡¨å¤´ï¼Œç»Ÿè®¡å”¯ä¸€å€¼
    if unique_values == 2:
        model_type = "classification"
        labels = list(target_col[1:].unique())  # è·å–ä¸¤ä¸ªå”¯ä¸€æ ‡ç­¾
        return model_type, labels
    else:
        model_type = "regression"
        return model_type, None

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
@st.cache_resource
def load_model():
    return tf.keras.models.load_model('data/MODEL.h5')

# å½’ä¸€åŒ–æ“ä½œ
norm_op_dict = {
    'ext': lambda data, param: data / param.get('max', 1),
    'avg': lambda data, param: data - param.get('mean', 0),
    'avgext': lambda data, param: (data - param.get('mean', 0)) / param.get('max', 1),
    'avgstd': lambda data, param: (data - param.get('mean', 0)) / param.get('std', 1),
    'none': lambda data, param: data
}

# åå½’ä¸€åŒ–æ“ä½œ
denorm_op_dict = {
    'ext': lambda data, param: data * param.get('max', 1),
    'avg': lambda data, param: data + param.get('mean', 0),
    'avgext': lambda data, param: data * param.get('max', 1) + param.get('mean', 0),
    'avgstd': lambda data, param: data * param.get('std', 1) + param.get('mean', 0),
    'none': lambda data, param: data
}

# åˆå§‹åŒ–æ•°æ®å’Œæ¨¡å‹
background_data, norm_params = load_background_data()
if background_data is None:
    st.stop()

model = load_model()
norm_method = load_norm_method()

# é»˜è®¤ç‰¹å¾å€¼ï¼ˆä½¿ç”¨åŸå§‹æœªå½’ä¸€åŒ–å€¼ï¼‰
default_values = background_data.iloc[0, :].to_dict()

# ç¡®å®šæ¨¡å‹ç±»å‹
model_type, class_labels = determine_model_type()
if model_type is None:
    st.stop()

# ä¾§è¾¹æ é…ç½®
st.sidebar.header("ç‰¹å¾è¾“å…¥")
st.sidebar.markdown("è°ƒæ•´ç‰¹å¾å€¼ï¼š")

# é‡ç½®æŒ‰é’®
if st.sidebar.button("é‡ç½®ä¸ºé»˜è®¤å€¼", key="reset"):
    st.session_state.update(default_values)

features = list(default_values.keys())
values = {}
cols = st.sidebar.columns(2)

for i, feature in enumerate(features):
    with cols[i % 2]:
        values[feature] = st.number_input(
            feature,
            min_value=float(background_data[feature].min()),
            max_value=float(background_data[feature].max()),
            value=default_values[feature],
            step=0.001,
            format="%.3f",
            key=feature
        )

# å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆå½’ä¸€åŒ–ï¼‰
def prepare_input_data():
    input_df = pd.DataFrame([values])
    norm_func = norm_op_dict.get(norm_method, norm_op_dict['none'])
    normalized_data = input_df.copy()
    for col in input_df.columns:
        normalized_data[col] = norm_func(input_df[col], {
            'mean': norm_params['mean'].get(col, 0),
            'max': norm_params['max'].get(col, 1),
            'std': norm_params['std'].get(col, 1)
        })
    return normalized_data, input_df

# ä¸»åˆ†æ
if st.button("åˆ†æè®¡ç®—", key="calculate"):
    normalized_input_df, original_input_df = prepare_input_data()

    # é¢„æµ‹
    prediction = model.predict(normalized_input_df.values, verbose=0)[0][0]

    # å¯¹äºå›å½’æ¨¡å‹ï¼Œåå½’ä¸€åŒ–é¢„æµ‹å€¼
    if model_type == "regression":
        denorm_func = denorm_op_dict.get(norm_method, denorm_op_dict['none'])
        excel_file = get_excel_file()
        df_y = pd.read_excel(excel_file).iloc[:, -1]  # ç›®æ ‡åˆ—
        target_params = {
            'mean': df_y.mean(),
            'max': df_y.max(),
            'std': df_y.std()
        }
        display_prediction = denorm_func(prediction, target_params)
    else:
        display_prediction = prediction  # åˆ†ç±»æ¨¡å‹ï¼šä½¿ç”¨åŸå§‹æ¦‚ç‡

    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
    with st.container():
        st.header("ğŸ“ˆ é¢„æµ‹ç»“æœ")
        col1, col2 = st.columns(2)
        with col1:
            if model_type == "classification":
                predicted_class = class_labels[1] if prediction >= 0.5 else class_labels[0]
                st.metric(
                    "æ¦‚ç‡",
                    f"{prediction:.4f}",
                    delta=f"é¢„æµ‹ç±»åˆ«: {predicted_class}",
                    delta_color="inverse"
                )
            else:
                st.metric(
                    "é¢„æµ‹å€¼",
                    f"{display_prediction:.4f}"
                )
        with col2:
            if model_type == "classification":
                st.metric(
                    "åˆ†ç±»é˜ˆå€¼",
                    f"0.5"
                )
            else:
                excel_file = get_excel_file()
                df_y = pd.read_excel(excel_file).iloc[:, -1]
                st.metric(
                    "é¢„æµ‹èŒƒå›´",
                    f"{df_y.min():.2f} - {df_y.max():.2f}"
                )

    # SHAP è§£é‡Š
    explainer = shap.DeepExplainer(model, background_data.values)
    shap_values = np.squeeze(np.array(explainer.shap_values(normalized_input_df.values)))
    base_value = float(explainer.expected_value[0].numpy())

    # å¯è§†åŒ–æ ‡ç­¾é¡µ
    tab1, tab2, tab3 = st.tabs(["Force Plot", "Decision Plot", "æœºç†æ´å¯Ÿ"])

    with tab1:
        st.subheader("Force Plot")
        col1, col2 = st.columns([3, 1])
        with col1:
            explanation = shap.Explanation(
                values=shap_values,
                base_values=base_value,
                feature_names=original_input_df.columns,
                data=original_input_df.values.round(3)
            )
            shap.plots.force(explanation, matplotlib=True, show=False, figsize=(20, 4))
            st.pyplot(plt.gcf(), clear_figure=True)

    with tab2:
        st.subheader("Decision Plot")
        col1, col2 = st.columns([2, 2])
        with col1:
            fig, ax = plt.subplots(figsize=(6, 3))
            shap.decision_plot(base_value, shap_values, original_input_df.columns, show=False)
            st.pyplot(plt.gcf(), clear_figure=True)

    with tab3:
        st.subheader("æœºç†æ´å¯Ÿ")
        importance_df = pd.DataFrame({'Feature': original_input_df.columns, 'SHAP Value': shap_values})
        importance_df = importance_df.sort_values('SHAP Value', ascending=False)
        st.dataframe(importance_df.style.background_gradient(cmap='coolwarm', subset=['SHAP Value']))